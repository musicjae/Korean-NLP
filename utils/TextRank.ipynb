{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextRank.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO5PnFS9U9W7v4U9uEZfjyq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/musicjae/Korean-NLP/blob/main/utils/TextRank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq80lA-TmsT5"
      },
      "source": [
        "# Reference  \r\n",
        "[1] https://excelsior-cjh.tistory.com/93"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YgqFp2VmxPm"
      },
      "source": [
        "- 텍스트랭크: 페이지랭크를 기반으로 만든 알고리즘. 문서 내에서 문장 또는 단어 간 중요도를 고려하여 랭킹을 계산\r\n",
        "  - 페이지 랭크: 중요도가 높은 웹 페이지는 다른 많은 페이지로부터 링크를 받는다는 점에서 착안.\r\n",
        "    - PR(A) = (1-d) + d (PR(T1)/C(T1) + … + PR(Tn)/C(Tn))  \r\n",
        "        - PR(T1): T1의 PageRank value\r\n",
        "        - C(T1): T1이 갖는 링크의 개수  \r\n",
        "        - **함의**:‘어떤 페이지 A의 페이지 랭크는 그 페이지를 인용하고 있는 다른 페이지 T1, T2, T3, .. 가 가진 페이지 랭크를 정규화시킨 값의 합'\r\n",
        "        - 보통 d = 0.85 \r\n",
        "            - 85% 확률로 다른 페이지 클릭 / 15% 확률로 순간 클릭을 멈추고 그 페이지 살펴봄\r\n",
        "        - **문제점**: 위 방식 채택 시, PR의 합산은 모든 페이지 개수인 N이 됨.\r\n",
        "        - **위키피디아 제안**: 위 문제점 해소 위해, PR(A) = (1-d)/N + d (PR(T1)/C(T1) + … + PR(Tn)/C(Tn))가 적절한 식."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFJrFZVDo7mw"
      },
      "source": [
        "- TR: $TR(Vi)=(1−d)+d∗∑_{Vj∈In(Vi)}\\frac{w_{ji}}{∑_{Vk∈Out(Vj)}w_{jk}}TR(Vj)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTKF5h2npNrR"
      },
      "source": [
        "- TR(Vi): 문장 또는 단어(V)i에 대한 TextRank값\r\n",
        "- $w_{ij}$: 문장 또는 단어 i 와 j 사이의 가중치\r\n",
        "- d:damping factor, PageRank에서 웹 서핑을 하는 사람이 해당 페이지를 만족하지 못하고 다른페이지로 이동하는 확률, TextRank에서도 그 값을 그대로 사용(0.85로 설정)\r\n",
        "- TextRank TR(Vi)를 계산 한 뒤 높은 순으로 정렬\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4OCmQ7tmpqP",
        "outputId": "c0c295e5-bb49-4950-8b78-a9b4015bd416"
      },
      "source": [
        "!pip install konlpy\r\n",
        "!pip install newspaper3k\r\n",
        "\r\n",
        "from newspaper import Article\r\n",
        "from konlpy.tag import Kkma\r\n",
        "from konlpy.tag import Okt\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.preprocessing import normalize\r\n",
        "import numpy as np\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.7/dist-packages (0.2.8)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.0)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WQOyoBlp8Tv"
      },
      "source": [
        "class SentenceTokenizer(object):\r\n",
        "    def __init__(self):\r\n",
        "        self.kkma = Kkma()\r\n",
        "        self.twitter = Okt()\r\n",
        "        self.stopwords = ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\",\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\",]\r\n",
        "\r\n",
        "    def url2sentences(self, url):\r\n",
        "        article = Article(url, language='ko')\r\n",
        "        article.download()\r\n",
        "        article.parse()\r\n",
        "        sentences = self.kkma.sentences(article.text)\r\n",
        "\r\n",
        "        for idx in range(0, len(sentences)):\r\n",
        "            if len(sentences[idx]) <= 10:\r\n",
        "                sentences[idx-1] += (' ' + sentences[idx])\r\n",
        "                sentences[idx] = ''\r\n",
        "\r\n",
        "        return sentences\r\n",
        "\r\n",
        "    def text2sentences(self, text):\r\n",
        "        sentences = self.kkma.sentences(text)\r\n",
        "        for idx in range(0, len(sentences)):\r\n",
        "            if len(sentences[idx]) <= 10:\r\n",
        "                sentences[idx-1] += (' ' + sentences[idx])\r\n",
        "                sentences[idx] = ''\r\n",
        "        return sentences\r\n",
        "\r\n",
        "\r\n",
        "    def get_nouns(self, sentences):\r\n",
        "        nouns = []\r\n",
        "        for sentence in sentences:\r\n",
        "            if sentence is not '':\r\n",
        "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence)) if noun not in self.stopwords and len(noun) > 1]))\r\n",
        "        return nouns\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86WQyraPqm1p"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW4YOhj_qoTY"
      },
      "source": [
        "- TF: 특정 단어가 문서 내에 얼만큼 나타나는지 수치\r\n",
        "- IDF: 전체 문서 수 / 해당 단어가 나타난 문서 수\r\n",
        "  - DF의 역수 사용 이유: DF가 가장 큰 값을 1로 만들어주기 위함 (확률 처리)\r\n",
        "\r\n",
        "- Adjacency Mat: Correlation Mat는 sen-term Mat의 전치와 원행렬을 곱하여 구함. 이것이 곧 Adj mat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eritYASKqnsc"
      },
      "source": [
        "class GraphMatrix(object):\r\n",
        "    \r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        self.tfidf = TfidfVectorizer()\r\n",
        "        self.cnt_vec = CountVectorizer()\r\n",
        "        self.graph_sentence = []\r\n",
        "\r\n",
        "\r\n",
        "    def build_sent_graph(self, sentence):\r\n",
        "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\r\n",
        "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\r\n",
        "        return self.graph_sentence\r\n",
        "\r\n",
        "\r\n",
        "    def build_words_graph(self, sentence):\r\n",
        "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\r\n",
        "        vocab = self.cnt_vec.vocabulary_\r\n",
        "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuRzMveOryXU"
      },
      "source": [
        "class Rank(object):\r\n",
        "    \r\n",
        "    def get_ranks(self, graph, d=0.85): # d = damping factor\r\n",
        "        A = graph\r\n",
        "        matrix_size = A.shape[0]\r\n",
        "        for id in range(matrix_size):\r\n",
        "            A[id, id] = 0 # diagonal 부분을 0으로\r\n",
        "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\r\n",
        "            if link_sum != 0:\r\n",
        "                A[:, id] /= link_sum\r\n",
        "            A[:, id] *= -d\r\n",
        "            A[id, id] = 1\r\n",
        "\r\n",
        "        B = (1-d) * np.ones((matrix_size, 1))\r\n",
        "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\r\n",
        "        return {idx: r[0] for idx, r in enumerate(ranks)}\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmB8r_4xsD9H"
      },
      "source": [
        "class TextRank(object):\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, text):\r\n",
        "        self.sent_tokenize = SentenceTokenizer()\r\n",
        "        if text[:5] in ('http:', 'https'):\r\n",
        "            self.sentences = self.sent_tokenize.url2sentences(text)\r\n",
        "        else:\r\n",
        "            self.sentences = self.sent_tokenize.text2sentences(text)\r\n",
        "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\r\n",
        "        self.graph_matrix = GraphMatrix()\r\n",
        "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\r\n",
        "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\r\n",
        "        self.rank = Rank()\r\n",
        "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\r\n",
        "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\r\n",
        "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\r\n",
        "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\r\n",
        "\r\n",
        "\r\n",
        "    def summarize(self, sent_num=3):\r\n",
        "        summary = []\r\n",
        "        index=[]\r\n",
        "        for idx in self.sorted_sent_rank_idx[:sent_num]:\r\n",
        "            index.append(idx)\r\n",
        "        index.sort()\r\n",
        "        for idx in index:\r\n",
        "            summary.append(self.sentences[idx])\r\n",
        "        return summary\r\n",
        "    \r\n",
        "    \r\n",
        "    def keywords(self, word_num=10):\r\n",
        "        rank = Rank()\r\n",
        "        rank_idx = rank.get_ranks(self.words_graph)\r\n",
        "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\r\n",
        "        keywords = []\r\n",
        "        index=[]\r\n",
        "        for idx in sorted_rank_idx[:word_num]:\r\n",
        "            index.append(idx)\r\n",
        "        #index.sort()\r\n",
        "        for idx in index:\r\n",
        "            keywords.append(self.idx2word[idx])\r\n",
        "        return keywords\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd3-iZ2csYI5"
      },
      "source": [
        "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=102&oid=055&aid=0000877970'\r\n",
        "textrank = TextRank(url)\r\n",
        "for row in textrank.summarize(3):\r\n",
        "    print(row)\r\n",
        "    print()\r\n",
        "print('keywords :',textrank.keywords())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}